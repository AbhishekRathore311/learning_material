{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Embedding, Linear, ReLU, GRU\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urllib.parse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        ###############################################\n",
    "        # Replace with your own \"guess\" function here #\n",
    "        ###############################################\n",
    "\n",
    "        # clean the word so that we strip away the space characters\n",
    "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
    "        clean_word = word[::2].replace(\"_\",\".\")\n",
    "        \n",
    "        # find length of passed word\n",
    "        len_word = len(clean_word)\n",
    "        \n",
    "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
    "        current_dictionary = self.current_dictionary\n",
    "        new_dictionary = []\n",
    "        \n",
    "        # iterate through all of the words in the old plausible dictionary\n",
    "        for dict_word in current_dictionary:\n",
    "            # continue if the word is not of the appropriate length\n",
    "            if len(dict_word) != len_word:\n",
    "                continue\n",
    "                \n",
    "            # if dictionary word is a possible match then add it to the current dictionary\n",
    "            if re.match(clean_word,dict_word):\n",
    "                new_dictionary.append(dict_word)\n",
    "        \n",
    "        # overwrite old possible words dictionary with updated version\n",
    "        self.current_dictionary = new_dictionary\n",
    "        \n",
    "        \n",
    "        # count occurrence of all characters in possible word matches\n",
    "        full_dict_string = \"\".join(new_dictionary)\n",
    "        \n",
    "        c = collections.Counter(full_dict_string)\n",
    "        sorted_letter_count = c.most_common()                   \n",
    "        \n",
    "        guess_letter = '!'\n",
    "        \n",
    "        # return most frequently occurring letter in all possible words that hasn't been guessed yet\n",
    "        for letter,instance_count in sorted_letter_count:\n",
    "            if letter not in self.guessed_letters:\n",
    "                guess_letter = letter\n",
    "                break\n",
    "            \n",
    "        # if no word matches in training dictionary, default back to ordering of full dictionary\n",
    "        if guess_letter == '!':\n",
    "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
    "            for letter,instance_count in sorted_letter_count:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    guess_letter = letter\n",
    "                    break            \n",
    "        \n",
    "        return guess_letter\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_words_location = \"data/words_250000_train.txt\"\n",
    "with open(development_words_location, \"r\") as fp:\n",
    "    development_words = fp.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181840 45460\n"
     ]
    }
   ],
   "source": [
    "train_words, test_words = train_test_split(development_words, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(len(train_words), len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exhilarating', 'clonic', 'semiphenomenally', 'preascertaining', 'benoit']\n",
      "['timpani', 'worsle', 'yinst', 'grangerized', 'matatua']\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:5])\n",
    "print(test_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = \"data/train_words.txt\"\n",
    "with open(train_location, \"w\") as fp:\n",
    "    for train_word in train_words:\n",
    "        fp.write(f\"{train_word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_location = \"data/test_words.txt\"\n",
    "with open(test_location, \"w\") as fp:\n",
    "    for test_word in test_words:\n",
    "        fp.write(f\"{test_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        gru_hidden_dim,\n",
    "        gru_num_layers,\n",
    "        char_embedding_dim,\n",
    "        missed_char_linear_dim,\n",
    "        nn_hidden_dim,\n",
    "        gru_dropout,\n",
    "        learning_rate\n",
    "    ):\n",
    "        super(HangmanGRU, self).__init__()\n",
    "\n",
    "        ## Different model dimentions\n",
    "        self.gru_hidden_dim = gru_hidden_dim \n",
    "        self.gru_num_layers = gru_num_layers\n",
    "\n",
    "        ## Embedding layer for character input\n",
    "        self.embedding = Embedding(vocab_size + 1, char_embedding_dim)\n",
    "\n",
    "        ## Missed characters linear layer\n",
    "        self.missed_characters_linear_layer = Linear(vocab_size, missed_char_linear_dim) \n",
    "\n",
    "        ## Declare GRU\n",
    "        self.hangman_gru = GRU(\n",
    "            input_size = char_embedding_dim, \n",
    "            hidden_size = self.gru_hidden_dim, \n",
    "            num_layers = self.gru_num_layers,\n",
    "            dropout = gru_dropout,\n",
    "            bidirectional=True, \n",
    "            batch_first=True\n",
    "        )\n",
    "            \n",
    "        # NN after GRU output\n",
    "        nn_in_features = missed_char_linear_dim + (self.gru_hidden_dim * 2)\n",
    "        self.nn_hidden_layer = Linear(nn_in_features, nn_hidden_dim)\n",
    "        self.relu = ReLU()\n",
    "        self.nn_output_layer = Linear(nn_hidden_dim, vocab_size)\n",
    "\n",
    "        ## Set up optimizer\n",
    "        self.optimizer = Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, x_lenths, missed_characters):\n",
    "        x = self.embedding(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = pack_padded_sequence(x, x_lenths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        ## Run through GRU\n",
    "        output, hidden = self.hangman_gru(x)\n",
    "        hidden = hidden.view(self.gru_num_layers, 2, -1, self.gru_hidden_dim)\n",
    "        hidden = hidden[-1]\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
    "\n",
    "        ## Project missed_characters to higher dimension\n",
    "        missed_characters = self.missed_characters_linear_layer(missed_characters)\n",
    "        \n",
    "        ## Concatenate GRU output and missed_characters\n",
    "        concatenated = torch.cat((hidden, missed_characters), dim=1)\n",
    "        \n",
    "        ## Run NN after GRU\n",
    "        nn_output = self.nn_hidden_layer(concatenated)\n",
    "        nn_output = self.relu(nn_output)\n",
    "        nn_output = self.nn_output_layer(nn_output)\n",
    "        return nn_output\n",
    "\n",
    "    def calculate_loss(self, model_out, labels, input_lengths, missed_characters, use_cuda):\n",
    "        outputs = nn.functional.log_softmax(model_out, dim=1)\n",
    "        \n",
    "        ## Calculate model output loss for miss characters\n",
    "        miss_penalty = torch.sum((outputs * missed_characters), dim=(0,1))/outputs.shape[0]\n",
    "        \n",
    "        ## Convert input lengths to float\n",
    "        input_lengths = input_lengths.float()\n",
    "        \n",
    "        ## Weights per example is inversely proportional to length of word\n",
    "        ## This is because shorter words are harder to predict due to higher chances of missing a character\n",
    "        weights_orig = (1/input_lengths)/torch.sum(1/input_lengths).unsqueeze(-1)\n",
    "        weights = torch.zeros((weights_orig.shape[0], 1))    \n",
    "        \n",
    "        ## Resize so that torch can process it correctly\n",
    "        weights[:, 0] = weights_orig\n",
    "\n",
    "        if use_cuda:\n",
    "            weights = weights.cuda()\n",
    "        \n",
    "        ## Actual Loss\n",
    "        loss_function = nn.BCEWithLogitsLoss(weight=weights, reduction='sum')\n",
    "        actual_penalty = loss_function(model_out, labels)\n",
    "        return actual_penalty, miss_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_encode(word, vocab_size, min_allowed_word_lengh, char_to_id):\n",
    "\t## Remove spaces, small words and make the word into lower case\n",
    "\tword = word.strip().lower()\n",
    "\tif len(word) < min_allowed_word_lengh:\n",
    "\t\treturn None, None, None\n",
    "\n",
    "\tencoded_word = np.zeros((len(word), vocab_size + 1))\n",
    "\t\n",
    "\t## Char location dict\n",
    "\t## For Ex 'goto', char_location_dict = {'g_id':[0], 'o_id':[1, 3], 't_id':[2]}\n",
    "\tchar_location_dict = {k: [] for k in range(vocab_size)}\n",
    "\n",
    "\tfor i, c in enumerate(word):\n",
    "\t\tidx = char_to_id[c]\n",
    "\t\tchar_location_dict[idx].append(i)\n",
    "\t\tencoded_word[i][idx] = 1\n",
    "\n",
    "\t## Char location list\n",
    "\t## For Ex 'goto', char_location_list = [[0], [1, 3], [2]]\n",
    "\tchar_location_list = [x for x in char_location_dict.values() if(len(x) > 0)]\n",
    "\n",
    "\t## word_set\n",
    "\t## For Ex 'goto', word_set = {'g', 'o', 't'}\n",
    "\tword_set = set(list(word))\n",
    "\treturn encoded_word, char_location_list, word_set\n",
    "\n",
    "def get_one_hot_encoded_words(\n",
    "    word_list,\n",
    "    vocab_size,\n",
    "    min_allowed_word_lengh\n",
    "):\n",
    "    char_to_id = {chr(97+x): x for x in range(vocab_size)}\n",
    "    char_to_id['BLANK'] = vocab_size\n",
    "    encoded_word_list = []\n",
    "    for word in word_list:\n",
    "        encoded_word, char_location_list, word_set = filter_and_encode(word, vocab_size, min_allowed_word_lengh, char_to_id)\n",
    "        if encoded_word is not None:\n",
    "            encoded_word_list.append((encoded_word, char_location_list, word_set))\n",
    "    return encoded_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Encoded Train Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exhilarating', 'clonic', 'semiphenomenally', 'preascertaining', 'benoit']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Encoded Train Words\n",
    "train_words_location = \"data/train_words.txt\"\n",
    "with open(train_words_location, \"r\") as fp:\n",
    "    train_words = fp.read().splitlines()\n",
    "train_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[5, 7], [0], [11], [2], [3, 9], [4], [10], [6], [8], [1]],\n",
       "  {'a', 'e', 'g', 'h', 'i', 'l', 'n', 'r', 't', 'x'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_words = get_one_hot_encoded_words(\n",
    "    word_list = train_words,\n",
    "    vocab_size = 26,\n",
    "    min_allowed_word_lengh = 3\n",
    ")\n",
    "encoded_train_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181840 181622\n"
     ]
    }
   ],
   "source": [
    "print(len(train_words), len(encoded_train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_words_location = \"data/encoded_train_words.pickle\"\n",
    "with open(encoded_train_words_location, \"wb\") as fp:\n",
    "    pickle.dump(encoded_train_words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Encoded Test Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timpani', 'worsle', 'yinst', 'grangerized', 'matatua']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Encoded Test Words\n",
    "test_words_location = \"data/test_words.txt\"\n",
    "with open(test_words_location, \"r\") as fp:\n",
    "    test_words = fp.read().splitlines()\n",
    "test_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45460 45397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[4], [1, 6], [2], [5], [3], [0]],\n",
       "  {'a', 'i', 'm', 'n', 'p', 't'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_words = get_one_hot_encoded_words(\n",
    "    word_list = test_words,\n",
    "    vocab_size = 26,\n",
    "    min_allowed_word_lengh = 3\n",
    ")\n",
    "print(len(test_words), len(encoded_test_words))\n",
    "encoded_test_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_words_location = \"data/encoded_test_words.pickle\"\n",
    "with open(encoded_test_words_location, \"wb\") as fp:\n",
    "    pickle.dump(encoded_test_words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get current epoch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_epoch_data(\n",
    "\tencoded_word_list, \n",
    "\tepoch_number, \n",
    "\ttotal_epochs,\n",
    "\tvocab_size\n",
    "):\n",
    "\t## As training progresses the prob of dropping chars increases using sigmoid on epoch\n",
    "\tdrop_char_probability = 1/(1+np.exp(-epoch_number/total_epochs))\n",
    "\tcur_epoch_data_list = []\n",
    "\tall_character_set = set([chr(97+x) for x in range(vocab_size)])\n",
    "\tchar_to_id = {chr(97+x): x for x in range(vocab_size)}\n",
    "\tchar_to_id['BLANK'] = vocab_size\n",
    "\n",
    "\tfor i, (encoded_word, char_location_list, word_set) in enumerate(encoded_word_list):\n",
    "\t\t## Number of characters to drop\n",
    "\t\tnum_char_to_drop = np.random.binomial(len(char_location_list), drop_char_probability)\n",
    "\t\tif num_char_to_drop == 0:\n",
    "\t\t\tnum_char_to_drop = 1\n",
    "\n",
    "\t\t## Drop chars inversely proportional to number of occurences of each character\n",
    "\t\t## For Ex: goto, char_location_list = [[0], [1, 3], [2]]\n",
    "\t\t## drop_char_probability_list = [0.4, 0.2, 0.4]\n",
    "\t\t## to_drop = [0, 1]\n",
    "\t\tdrop_char_probability_list = [1/len(x) for x in char_location_list]\n",
    "\t\tdrop_char_probability_list = [x/sum(drop_char_probability_list) for x in drop_char_probability_list]\n",
    "\t\tto_drop = np.random.choice(len(char_location_list), num_char_to_drop, p=drop_char_probability_list, replace=False)\n",
    "\n",
    "\t\t## Cha positions to drop\n",
    "\t\t## For Ex: goto, char_location_list = [[0], [1, 3], [2]] and to_drop = [0, 1]\n",
    "\t\t## drop_char_idx = [0, 1, 3]\n",
    "\t\tdrop_char_idx = []\n",
    "\t\tfor char_group in to_drop:\n",
    "\t\t\tdrop_char_idx += char_location_list[char_group]\n",
    "\t\t\n",
    "\t\t## drop_char_idx = model target\n",
    "\t\t## Assuming voab_size = 4\n",
    "\t\t## For Ex: goto, encoded_word = [[1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
    "\t\t## unclipped_target = [1, 0, 2, 0]\n",
    "\t\t## target = [1, 0, 1, 0]\n",
    "\t\tunclipped_target = np.sum(encoded_word[drop_char_idx], axis=0)\n",
    "\t\ttarget = np.clip(unclipped_target, 0, 1)\n",
    "\n",
    "\t\t## Remove blank in target\n",
    "\t\ttarget = target[:-1]\n",
    "\t\t\n",
    "\t\t## Drop chars and assign blank_character\n",
    "\t\tinput_vec = np.copy(encoded_word)\n",
    "\t\tblank_vec = np.zeros((1, vocab_size + 1))\n",
    "\t\tblank_vec[0, vocab_size] = 1\n",
    "\t\tinput_vec[drop_char_idx] = blank_vec\n",
    "\n",
    "\t\t## Provide character id instead of 1-hot encoded vector for embedding\n",
    "\t\tinput_vec = np.argmax(input_vec, axis=1)\n",
    "\t\t## For Ex: goto, encoded_word = [[1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
    "\t\t## drop_char_idx = [0, 1, 3]\n",
    "\t\t## target = [1, 0, 1, 0]\n",
    "\t\t## input_vec = [26, 26, 19, 26] (26 = BLANK, 19 = t)\n",
    "\t\t\n",
    "\t\t## randomly pick a few characters from vocabulary as characters which were predicted but declared as not present by game\n",
    "\t\tnot_present_char_sorted_array = np.array(sorted(list(all_character_set - word_set)))\n",
    "\t\tnum_missed_chars = np.random.randint(0, 10)\n",
    "\t\tmiss_char_sorted_array = np.random.choice(not_present_char_sorted_array, num_missed_chars)\n",
    "\t\tmiss_char_id_sorted_list = [char_to_id[x] for x in miss_char_sorted_array]\n",
    "\t\t## Ex word is 'goto', num_missed_chars = 2, miss_char_id_sorted_list = [1, 3] \n",
    "\t\t## (which correspond to the characters b and d)\n",
    "\t\t\n",
    "\t\tmiss_vec = np.zeros(vocab_size)\n",
    "\t\tmiss_vec[miss_char_id_sorted_list] = 1\n",
    "\t\t## If vocab_size = 6, b = 1, d = 3 and b, d are missed\n",
    "\t\t## miss_vec = [0, 1, 0, 1, 0, 0]\n",
    "\t\t\n",
    "\t\t## Append tuple to cur_epoch_data_list\n",
    "\t\tcur_epoch_data_list.append((input_vec, target, miss_vec))\n",
    "\n",
    "\t## Shuffle dataset before feeding batches to the model\n",
    "\tnp.random.shuffle(cur_epoch_data_list)\n",
    "\treturn cur_epoch_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test current epoch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[5, 7], [0], [11], [2], [3, 9], [4], [10], [6], [8], [1]],\n",
       "  {'a', 'e', 'g', 'h', 'i', 'l', 'n', 'r', 't', 'x'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_words_location = \"data/encoded_train_words.pickle\"\n",
    "encoded_train_word_list = pickle.load(open(encoded_train_words_location, \"rb\"))\n",
    "encoded_train_word_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([26, 26, 22, 26, 26, 26, 18]),\n",
       "  array([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 1., 1.]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_epoch_train_data_list = get_current_epoch_data(\n",
    "\tencoded_word_list = encoded_train_word_list, \n",
    "\tepoch_number = 24, \n",
    "\ttotal_epochs = 100,\n",
    "\tvocab_size = 26\n",
    ")\n",
    "cur_epoch_train_data_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181622, 181622)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train_word_list), len(cur_epoch_train_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get current batch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_words(input_vec_list, vocab_size):\n",
    "    total_seq = len(input_vec_list)\n",
    "    max_len = max([len(x) for x in input_vec_list])\n",
    "    batched_input_list = []\n",
    "\n",
    "    for word in input_vec_list:\n",
    "        if max_len != len(word):\n",
    "            ## Add blanks to get max len\n",
    "            blank_vec = (vocab_size * np.ones((max_len - word.shape[0])))\n",
    "            word = np.concatenate((word, blank_vec), axis=0)\n",
    "        batched_input_list.append(word)\n",
    "\n",
    "    return np.array(batched_input_list)\n",
    "\n",
    "def get_cur_batch_data(\n",
    "    cur_epoch_data_list, \n",
    "    batch_id, \n",
    "    batch_size,\n",
    "    vocab_size\n",
    "):\n",
    "    if(((batch_id + 1) * batch_size) <= len(cur_epoch_data_list)):\n",
    "        start_index = (batch_id * batch_size)\n",
    "        end_index = ((batch_id + 1) * batch_size)\n",
    "        cur_batch_data_list = cur_epoch_data_list[start_index: end_index]\n",
    "    else:\n",
    "        start_index = (batch_id * batch_size)\n",
    "        end_index = len(cur_epoch_data_list)\n",
    "        cur_batch_data_list = cur_epoch_data_list[start_index: end_index]\n",
    "    \n",
    "    ## Convert to numpy arrays\n",
    "    word_length_array = np.array([len(x[0]) for x in cur_batch_data_list])\n",
    "    input_vec_list = [x[0] for x in cur_batch_data_list]\n",
    "    batched_input_array = batchify_words(input_vec_list, vocab_size)\n",
    "    batched_label_array = np.array([x[1] for x in cur_batch_data_list])\n",
    "    batched_missed_char_array = np.array([x[2] for x in cur_batch_data_list])\n",
    "\n",
    "    ## Return batch\n",
    "    return batched_input_array, batched_label_array, batched_missed_char_array, word_length_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Get current batch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_array, batched_label_array, batched_missed_char_array, word_length_array = get_cur_batch_data(\n",
    "    cur_epoch_data_list = cur_epoch_train_data_list, \n",
    "    batch_id = 2, \n",
    "    batch_size = 4000,\n",
    "    vocab_size = 26\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000 4000 4000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(batched_input_array),\n",
    "    len(batched_label_array),\n",
    "    len(batched_missed_char_array),\n",
    "    len(word_length_array)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19., 26., 26., 26., 26., 20., 26., 26., 26., 26., 26., 26., 26.,\n",
       "        26., 26., 26., 26., 26., 26., 26., 26., 26., 26.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_input_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_label_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_missed_char_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  4, 14,  9,  9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Bi Direnctional GRU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "\tepoch,\n",
    "\tmodel,\n",
    "\ttotal_epochs,\n",
    "\tencoded_test_word_list,\n",
    "\tbatch_size,\n",
    "\tvocab_size,\n",
    "\tcuda\n",
    "):\n",
    "\tmodel.eval()\n",
    "\n",
    "\t## Initialize epoch loss\n",
    "\ttest_loss = 0.0\n",
    "\ttest_miss_penalty = 0.0\n",
    "\n",
    "\t## Without gradient update\n",
    "\twith torch.no_grad():\n",
    "\t\t## Get cur_epoch_train_data_list\n",
    "\t\tcur_epoch_test_data_list = get_current_epoch_data(\n",
    "\t\t\tencoded_word_list = encoded_test_word_list, \n",
    "\t\t\tepoch_number = epoch,\n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tvocab_size = vocab_size\n",
    "\t\t)\n",
    "\n",
    "\t\t## Loop over batches\n",
    "\t\tno_batches = int(math.ceil(len(cur_epoch_test_data_list) / batch_size))\n",
    "\t\tfor batch_id in range(no_batches):\n",
    "\t\t\t## Get batch\n",
    "\t\t\tinputs, labels, miss_chars, input_lengths = get_cur_batch_data(\n",
    "\t\t\t\tcur_epoch_data_list = cur_epoch_test_data_list, \n",
    "\t\t\t\tbatch_id = batch_id, \n",
    "\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\tvocab_size = vocab_size\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t## Embeddings should be of dtype long\n",
    "\t\t\tinputs = torch.from_numpy(inputs).long()\n",
    "\t\t\t\n",
    "\t\t\t## Convert to torch tensors\n",
    "\t\t\tlabels = torch.from_numpy(labels).float()\n",
    "\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
    "\t\t\tinput_lengths = torch.from_numpy(input_lengths).long()\n",
    "\n",
    "\t\t\tif(cuda==True):\n",
    "\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\tlabels = labels.cuda()\n",
    "\t\t\t\tmiss_chars = miss_chars.cuda()\n",
    "\t\t\t\tinput_lengths = input_lengths.cuda()\n",
    "\n",
    "\t\t\t# zero the parameter gradients\n",
    "\t\t\tmodel.optimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\t# Forward Pass\n",
    "\t\t\toutputs = model(inputs, input_lengths, miss_chars)\n",
    "\t\t\tloss, miss_penalty = model.calculate_loss(outputs, labels, input_lengths, miss_chars, cuda)\n",
    "\t\t\ttest_loss += loss.item()\n",
    "\t\t\ttest_miss_penalty += miss_penalty.item()\n",
    "\n",
    "\t# Average out the losses\n",
    "\ttest_loss = (test_loss / no_batches)\n",
    "\ttest_miss_penalty = (test_miss_penalty / no_batches)\n",
    "\treturn test_loss, test_miss_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bi Direnctional GRU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\ttotal_epochs,\n",
    "\tencoded_train_words_location,\n",
    "\tencoded_test_words_location,\n",
    "\tbatch_size,\n",
    "\tvocab_size,\n",
    "\tcuda,\n",
    "\tsave_every,\n",
    "\tmodel_output_location,\n",
    "\tgru_hidden_dim = 512,\n",
    "\tgru_num_layers = 2,\n",
    "\tchar_embedding_dim = 128,\n",
    "\tmissed_char_linear_dim = 256,\n",
    "\tnn_hidden_dim = 256,\n",
    "\tgru_dropout = 0.3,\n",
    "\tlearning_rate = 0.0005\n",
    "):\n",
    "\t## Load model and set it to train mode\n",
    "\tmodel = HangmanGRU(\n",
    "\t\tvocab_size = vocab_size,\n",
    "        gru_hidden_dim = gru_hidden_dim,\n",
    "        gru_num_layers = gru_num_layers,\n",
    "        char_embedding_dim = char_embedding_dim,\n",
    "        missed_char_linear_dim = missed_char_linear_dim,\n",
    "        nn_hidden_dim = nn_hidden_dim,\n",
    "        gru_dropout = gru_dropout,\n",
    "        learning_rate = learning_rate\n",
    "\t)\n",
    "\tmodel.train()\n",
    "\n",
    "\t## Get encoded_train_word_list\n",
    "\tencoded_train_word_list = pickle.load(open(encoded_train_words_location, \"rb\"))\n",
    "\t\n",
    "\t## Get encoded_test_word_list\n",
    "\tencoded_test_word_list = pickle.load(open(encoded_test_words_location, \"rb\"))\n",
    "\n",
    "\t## Lists to store losses\n",
    "\ttrain_loss_list = []\n",
    "\ttrain_miss_penalty_list = []\n",
    "\ttest_loss_list = []\n",
    "\ttest_miss_penalty_list = []\n",
    "\n",
    "\t## Loop over Train Data\n",
    "\tfor epoch in range(1, total_epochs+1):\n",
    "\t\t## Initialize epoch loss\n",
    "\t\ttrain_loss = 0.0\n",
    "\t\ttrain_miss_penalty = 0.0\n",
    "\n",
    "\t\t## Get cur_epoch_train_data_list\n",
    "\t\tcur_epoch_train_data_list = get_current_epoch_data(\n",
    "\t\t\tencoded_word_list = encoded_train_word_list, \n",
    "\t\t\tepoch_number = epoch, \n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tvocab_size = vocab_size\n",
    "\t\t)\n",
    "\n",
    "\t\t## Loop over batches\n",
    "\t\tno_batches = int(math.ceil(len(cur_epoch_train_data_list) / batch_size))\n",
    "\t\tfor batch_id in range(no_batches):\n",
    "\t\t\t## Get batch\n",
    "\t\t\tinputs, labels, miss_chars, input_lengths = get_cur_batch_data(\n",
    "\t\t\t\tcur_epoch_data_list = cur_epoch_train_data_list, \n",
    "\t\t\t\tbatch_id = batch_id, \n",
    "\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\tvocab_size = vocab_size\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t## Embeddings should be of dtype long\n",
    "\t\t\tinputs = torch.from_numpy(inputs).long()\n",
    "\t\t\t\n",
    "\t\t\t## Convert to torch tensors\n",
    "\t\t\tlabels = torch.from_numpy(labels).float()\n",
    "\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
    "\t\t\tinput_lengths = torch.from_numpy(input_lengths).long()\n",
    "\n",
    "\t\t\tif(cuda==True):\n",
    "\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\tlabels = labels.cuda()\n",
    "\t\t\t\tmiss_chars = miss_chars.cuda()\n",
    "\t\t\t\tinput_lengths = input_lengths.cuda()\n",
    "\n",
    "\t\t\t## Zero the parameter gradients\n",
    "\t\t\tmodel.optimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\t## Forward Pass, Loss calculation, Backward Pass, Optimize\n",
    "\t\t\toutputs = model(inputs, input_lengths, miss_chars)\n",
    "\t\t\tloss, miss_penalty = model.calculate_loss(outputs, labels, input_lengths, miss_chars, cuda)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tmodel.optimizer.step()\n",
    "\n",
    "\t\t\t## store loss\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\t\ttrain_miss_penalty += miss_penalty.item()\n",
    "\n",
    "\t\t# Test model after epoch\n",
    "\t\ttest_loss, test_miss_penalty = test(\n",
    "\t\t\tepoch = epoch,\n",
    "\t\t\tmodel = model,\n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tencoded_test_word_list = encoded_test_word_list,\n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tvocab_size = vocab_size,\n",
    "\t\t\tcuda = cuda\n",
    "\t\t)\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# Store losses\n",
    "\t\ttrain_loss = (train_loss / no_batches)\n",
    "\t\ttrain_loss_list.append(train_loss)\n",
    "\t\ttrain_miss_penalty = (train_miss_penalty/ no_batches)\n",
    "\t\ttrain_miss_penalty_list.append(train_miss_penalty)\n",
    "\t\ttest_loss_list.append(test_loss)\n",
    "\t\ttest_miss_penalty_list.append(test_miss_penalty)\n",
    "\n",
    "\t\t# Save Losses\n",
    "\t\tdf_losses = pd.DataFrame(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"train_loss\": train_loss_list,\n",
    "\t\t\t\t\"train_miss_penalty\": train_miss_penalty_list,\n",
    "\t\t\t\t\"test_loss\": test_loss_list,\n",
    "\t\t\t\t\"test_miss_penalty\": test_miss_penalty_list\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\t\tdf_losses_location = f\"{model_output_location}/df_losses.csv\"\n",
    "\t\tdf_losses.to_csv(df_losses_location, index=False)\n",
    "\n",
    "\t\t# Save model\n",
    "\t\tif(epoch % save_every == 0):\n",
    "\t\t\tmodel_path = f\"{model_output_location}/models\"\n",
    "\t\t\tmodel_file_name = f\"{model_path}/model_epoch_{str(epoch).zfill(4)}.pth\"\n",
    "\t\t\ttorch.save({\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t'test_loss': test_loss,\n",
    "\t\t\t}, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bi Directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train(\n",
    "\ttotal_epochs = 10,\n",
    "\tencoded_train_words_location = \"data/encoded_train_words.pickle\",\n",
    "\tencoded_test_words_location = \"data/encoded_test_words.pickle\",\n",
    "\tbatch_size = 250000,\n",
    "\tvocab_size = 26,\n",
    "\tcuda = False,\n",
    "\tsave_every = 1,\n",
    "\tmodel_output_location = \"model_output\",\n",
    "\tgru_hidden_dim = 512,\n",
    "\tgru_num_layers = 2,\n",
    "\tchar_embedding_dim = 128,\n",
    "\tmissed_char_linear_dim = 256,\n",
    "\tnn_hidden_dim = 256,\n",
    "\tgru_dropout = 0.3,\n",
    "\tlearning_rate = 0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"INSERT_YOUR_TOKEN_HERE\", timeout=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api.start_game(practice=1,verbose=True)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = total_practice_successes / total_practice_runs\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    #api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "270af51b3687a98de993e3398f2a00195eb6c674711263c21cd6f4db7277b321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
