{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Embedding, Linear, ReLU, GRU\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_words_location = \"../../data/words_250000_train.txt\"\n",
    "with open(development_words_location, \"r\") as fp:\n",
    "    development_words = fp.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181840 45460\n"
     ]
    }
   ],
   "source": [
    "train_words, test_words = train_test_split(development_words, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(len(train_words), len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exhilarating', 'clonic', 'semiphenomenally', 'preascertaining', 'benoit']\n",
      "['timpani', 'worsle', 'yinst', 'grangerized', 'matatua']\n"
     ]
    }
   ],
   "source": [
    "print(train_words[:5])\n",
    "print(test_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = \"../../data/gru_model/train_words.txt\"\n",
    "with open(train_location, \"w\") as fp:\n",
    "    for train_word in train_words:\n",
    "        fp.write(f\"{train_word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_location = \"../../data/gru_model/test_words.txt\"\n",
    "with open(test_location, \"w\") as fp:\n",
    "    for test_word in test_words:\n",
    "        fp.write(f\"{test_word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        gru_hidden_dim,\n",
    "        gru_num_layers,\n",
    "        char_embedding_dim,\n",
    "        missed_char_linear_dim,\n",
    "        nn_hidden_dim,\n",
    "        gru_dropout,\n",
    "        learning_rate\n",
    "    ):\n",
    "        super(HangmanGRU, self).__init__()\n",
    "\n",
    "        ## Different model dimentions\n",
    "        self.gru_hidden_dim = gru_hidden_dim \n",
    "        self.gru_num_layers = gru_num_layers\n",
    "\n",
    "        ## Embedding layer for character input\n",
    "        self.embedding = Embedding(vocab_size + 1, char_embedding_dim)\n",
    "\n",
    "        ## Declare GRU\n",
    "        self.hangman_gru = GRU(\n",
    "            input_size = char_embedding_dim,\n",
    "            hidden_size = self.gru_hidden_dim,\n",
    "            num_layers = self.gru_num_layers,\n",
    "            dropout = gru_dropout,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        ## Missed characters linear layer\n",
    "        self.missed_characters_linear_layer = Linear(vocab_size, missed_char_linear_dim)\n",
    "            \n",
    "        # NN after GRU output\n",
    "        nn_in_features = missed_char_linear_dim + (self.gru_hidden_dim * 2)\n",
    "        self.nn_hidden_layer = Linear(nn_in_features, nn_hidden_dim)\n",
    "        self.relu = ReLU()\n",
    "        self.nn_output_layer = Linear(nn_hidden_dim, vocab_size)\n",
    "\n",
    "        ## Set up optimizer\n",
    "        self.optimizer = Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x, x_length, missed_characters):\n",
    "        x = self.embedding(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = pack_padded_sequence(x, x_length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        ## Run through GRU\n",
    "        output, hidden = self.hangman_gru(x)\n",
    "        hidden = hidden.view(self.gru_num_layers, 2, -1, self.gru_hidden_dim)\n",
    "        hidden = hidden[-1]\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
    "\n",
    "        ## Project missed_characters to higher dimension\n",
    "        missed_characters = self.missed_characters_linear_layer(missed_characters)\n",
    "        \n",
    "        ## Concatenate GRU output and missed_characters\n",
    "        concatenated = torch.cat((hidden, missed_characters), dim=1)\n",
    "        \n",
    "        ## Run NN after GRU\n",
    "        nn_output = self.nn_hidden_layer(concatenated)\n",
    "        nn_output = self.relu(nn_output)\n",
    "        nn_output = self.nn_output_layer(nn_output)\n",
    "        return nn_output\n",
    "\n",
    "    def calculate_loss(self, model_out, labels, input_lengths, missed_characters, use_cuda):\n",
    "        outputs = nn.functional.log_softmax(model_out, dim=1)\n",
    "        \n",
    "        ## Calculate model output loss for miss characters\n",
    "        miss_penalty = torch.sum((outputs * missed_characters), dim=(0,1))/outputs.shape[0]\n",
    "        \n",
    "        ## Convert input lengths to float\n",
    "        input_lengths = input_lengths.float()\n",
    "        \n",
    "        ## Weights per example is inversely proportional to length of word\n",
    "        ## This is because shorter words are harder to predict due to higher chances of missing a character\n",
    "        weights_orig = (1/input_lengths)/torch.sum(1/input_lengths).unsqueeze(-1)\n",
    "        weights = torch.zeros((weights_orig.shape[0], 1))    \n",
    "        \n",
    "        ## Resize so that torch can process it correctly\n",
    "        weights[:, 0] = weights_orig\n",
    "\n",
    "        if use_cuda:\n",
    "            weights = weights.cuda()\n",
    "        \n",
    "        ## Actual Loss\n",
    "        loss_function = nn.BCEWithLogitsLoss(weight=weights, reduction='sum')\n",
    "        actual_penalty = loss_function(model_out, labels)\n",
    "        return actual_penalty, miss_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_encode(\n",
    "    word,\n",
    "    vocab_size,\n",
    "    min_allowed_word_lengh, \n",
    "    char_to_id\n",
    "):\n",
    "\t## Remove spaces, small words and make the word into lower case\n",
    "\tword = word.strip().lower()\n",
    "\tif len(word) < min_allowed_word_lengh:\n",
    "\t\treturn None, None, None\n",
    "\n",
    "\tencoded_word = np.zeros((len(word), vocab_size + 1))\n",
    "\t\n",
    "\t## Char location dict\n",
    "\t## For Ex 'goto', char_location_dict = {'g_id':[0], 'o_id':[1, 3], 't_id':[2]}\n",
    "\tchar_location_dict = {k: [] for k in range(vocab_size)}\n",
    "\n",
    "\tfor i, c in enumerate(word):\n",
    "\t\tidx = char_to_id[c]\n",
    "\t\tchar_location_dict[idx].append(i)\n",
    "\t\tencoded_word[i][idx] = 1\n",
    "\n",
    "\t## Char location list\n",
    "\t## For Ex 'goto', char_location_list = [[0], [1, 3], [2]]\n",
    "\tchar_location_list = [x for x in char_location_dict.values() if(len(x) > 0)]\n",
    "\n",
    "\t## word_set\n",
    "\t## For Ex 'goto', word_set = {'g', 'o', 't'}\n",
    "\tword_set = set(list(word))\n",
    "\treturn encoded_word, char_location_list, word_set\n",
    "\n",
    "def get_one_hot_encoded_words(\n",
    "    word_list,\n",
    "    vocab_size,\n",
    "    min_allowed_word_lengh\n",
    "):\n",
    "    char_to_id = {chr(97+x): x for x in range(vocab_size)}\n",
    "    char_to_id['BLANK'] = vocab_size\n",
    "    encoded_word_list = []\n",
    "    for word in word_list:\n",
    "        encoded_word, char_location_list, word_set = filter_and_encode(word, vocab_size, min_allowed_word_lengh, char_to_id)\n",
    "        if encoded_word is not None:\n",
    "            encoded_word_list.append((encoded_word, char_location_list, word_set))\n",
    "    return encoded_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Encoded Train Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exhilarating', 'clonic', 'semiphenomenally', 'preascertaining', 'benoit']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Encoded Train Words\n",
    "train_words_location = \"../../data/gru_model/train_words.txt\"\n",
    "with open(train_words_location, \"r\") as fp:\n",
    "    train_words = fp.read().splitlines()\n",
    "train_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[5, 7], [0], [11], [2], [3, 9], [4], [10], [6], [8], [1]],\n",
       "  {'a', 'e', 'g', 'h', 'i', 'l', 'n', 'r', 't', 'x'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_words = get_one_hot_encoded_words(\n",
    "    word_list = train_words,\n",
    "    vocab_size = 26,\n",
    "    min_allowed_word_lengh = 3\n",
    ")\n",
    "encoded_train_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181840 181622\n"
     ]
    }
   ],
   "source": [
    "print(len(train_words), len(encoded_train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_words_location = \"../../data/gru_model/encoded_train_words.pickle\"\n",
    "with open(encoded_train_words_location, \"wb\") as fp:\n",
    "    pickle.dump(encoded_train_words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Encoded Test Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['timpani', 'worsle', 'yinst', 'grangerized', 'matatua']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Encoded Test Words\n",
    "test_words_location = \"../../data/gru_model/test_words.txt\"\n",
    "with open(test_words_location, \"r\") as fp:\n",
    "    test_words = fp.read().splitlines()\n",
    "test_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45460 45397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[4], [1, 6], [2], [5], [3], [0]],\n",
       "  {'a', 'i', 'm', 'n', 'p', 't'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_words = get_one_hot_encoded_words(\n",
    "    word_list = test_words,\n",
    "    vocab_size = 26,\n",
    "    min_allowed_word_lengh = 3\n",
    ")\n",
    "print(len(test_words), len(encoded_test_words))\n",
    "encoded_test_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_words_location = \"../../data/gru_model/encoded_test_words.pickle\"\n",
    "with open(encoded_test_words_location, \"wb\") as fp:\n",
    "    pickle.dump(encoded_test_words, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Encoded Test Words Fraction for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45460"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save Encoded Test Words Fraction\n",
    "test_words_location = \"../../data/gru_model/test_words.txt\"\n",
    "with open(test_words_location, \"r\") as fp:\n",
    "    test_words = fp.read().splitlines()\n",
    "len(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45460 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_words_fraction = get_one_hot_encoded_words(\n",
    "    word_list = test_words[:1001],\n",
    "    vocab_size = 26,\n",
    "    min_allowed_word_lengh = 3\n",
    ")\n",
    "print(len(test_words), len(encoded_test_words_fraction))\n",
    "len(encoded_test_words_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_words_fraction_location = \"../../data/gru_model/encoded_test_words_fraction.pickle\"\n",
    "with open(encoded_test_words_fraction_location, \"wb\") as fp:\n",
    "    pickle.dump(encoded_test_words_fraction, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get current epoch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_epoch_data(\n",
    "\tencoded_word_list, \n",
    "\tepoch_number, \n",
    "\ttotal_epochs,\n",
    "\tvocab_size\n",
    "):\n",
    "\t## As training progresses the prob of dropping chars increases using sigmoid on epoch\n",
    "\tdrop_char_probability = 1/(1+np.exp(-epoch_number/total_epochs))\n",
    "\tcur_epoch_data_list = []\n",
    "\tall_character_set = set([chr(97+x) for x in range(vocab_size)])\n",
    "\tchar_to_id = {chr(97+x): x for x in range(vocab_size)}\n",
    "\tchar_to_id['BLANK'] = vocab_size\n",
    "\n",
    "\tfor i, (encoded_word, char_location_list, word_set) in tqdm(enumerate(encoded_word_list)):\n",
    "\t\t## Number of characters to drop\n",
    "\t\tnum_char_to_drop = np.random.binomial(len(char_location_list), drop_char_probability)\n",
    "\t\tif num_char_to_drop == 0:\n",
    "\t\t\tnum_char_to_drop = 1\n",
    "\n",
    "\t\t## Drop chars inversely proportional to number of occurences of each character\n",
    "\t\t## For Ex: goto, char_location_list = [[0], [1, 3], [2]]\n",
    "\t\t## drop_char_probability_list = [0.4, 0.2, 0.4]\n",
    "\t\t## to_drop = [0, 1]\n",
    "\t\tdrop_char_probability_list = [1/len(x) for x in char_location_list]\n",
    "\t\tdrop_char_probability_list = [x/sum(drop_char_probability_list) for x in drop_char_probability_list]\n",
    "\t\tto_drop = np.random.choice(len(char_location_list), num_char_to_drop, p=drop_char_probability_list, replace=False)\n",
    "\n",
    "\t\t## Cha positions to drop\n",
    "\t\t## For Ex: goto, char_location_list = [[0], [1, 3], [2]] and to_drop = [0, 1]\n",
    "\t\t## drop_char_idx = [0, 1, 3]\n",
    "\t\tdrop_char_idx = []\n",
    "\t\tfor char_group in to_drop:\n",
    "\t\t\tdrop_char_idx += char_location_list[char_group]\n",
    "\t\t\n",
    "\t\t## drop_char_idx = model target\n",
    "\t\t## Assuming voab_size = 4\n",
    "\t\t## For Ex: goto, encoded_word = [[1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
    "\t\t## unclipped_target = [1, 0, 2, 0]\n",
    "\t\t## target = [1, 0, 1, 0]\n",
    "\t\tunclipped_target = np.sum(encoded_word[drop_char_idx], axis=0)\n",
    "\t\ttarget = np.clip(unclipped_target, 0, 1)\n",
    "\n",
    "\t\t## Remove blank in target\n",
    "\t\ttarget = target[:-1]\n",
    "\t\t\n",
    "\t\t## Drop chars and assign blank_character\n",
    "\t\tinput_vec = np.copy(encoded_word)\n",
    "\t\tblank_vec = np.zeros((1, vocab_size + 1))\n",
    "\t\tblank_vec[0, vocab_size] = 1\n",
    "\t\tinput_vec[drop_char_idx] = blank_vec\n",
    "\n",
    "\t\t## Provide character id instead of 1-hot encoded vector for embedding\n",
    "\t\tinput_vec = np.argmax(input_vec, axis=1)\n",
    "\t\t## For Ex: goto, encoded_word = [[1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
    "\t\t## drop_char_idx = [0, 1, 3]\n",
    "\t\t## target = [1, 0, 1, 0]\n",
    "\t\t## input_vec = [26, 26, 19, 26] (26 = BLANK, 19 = t)\n",
    "\t\t\n",
    "\t\t## randomly pick a few characters from vocabulary as characters which were predicted but declared as not present by game\n",
    "\t\tnot_present_char_sorted_array = np.array(sorted(list(all_character_set - word_set)))\n",
    "\t\tnum_missed_chars = np.random.randint(0, 10)\n",
    "\t\tmiss_char_sorted_array = np.random.choice(not_present_char_sorted_array, num_missed_chars)\n",
    "\t\tmiss_char_id_sorted_list = [char_to_id[x] for x in miss_char_sorted_array]\n",
    "\t\t## Ex word is 'goto', num_missed_chars = 2, miss_char_id_sorted_list = [1, 3] \n",
    "\t\t## (which correspond to the characters b and d)\n",
    "\t\t\n",
    "\t\tmiss_vec = np.zeros(vocab_size)\n",
    "\t\tmiss_vec[miss_char_id_sorted_list] = 1\n",
    "\t\t## If vocab_size = 6, b = 1, d = 3 and b, d are missed\n",
    "\t\t## miss_vec = [0, 1, 0, 1, 0, 0]\n",
    "\t\t\n",
    "\t\t## Append tuple to cur_epoch_data_list\n",
    "\t\tcur_epoch_data_list.append((input_vec, target, miss_vec))\n",
    "\n",
    "\t## Shuffle dataset before feeding batches to the model\n",
    "\tnp.random.shuffle(cur_epoch_data_list)\n",
    "\treturn cur_epoch_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test current epoch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [[5, 7], [0], [11], [2], [3, 9], [4], [10], [6], [8], [1]],\n",
       "  {'a', 'e', 'g', 'h', 'i', 'l', 'n', 'r', 't', 'x'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_words_location = \"../../data/gru_model/encoded_train_words.pickle\"\n",
    "encoded_train_word_list = pickle.load(open(encoded_train_words_location, \"rb\"))\n",
    "encoded_train_word_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "181622it [01:18, 2307.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([ 4, 26, 26,  8, 26,  0, 26,  0, 19,  8, 13, 26], dtype=int64),\n",
       "  array([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       "  array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.]))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_epoch_train_data_list = get_current_epoch_data(\n",
    "\tencoded_word_list = encoded_train_word_list, \n",
    "\tepoch_number = 24, \n",
    "\ttotal_epochs = 100,\n",
    "\tvocab_size = 26\n",
    ")\n",
    "cur_epoch_train_data_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181622, 181622)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_train_word_list), len(cur_epoch_train_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get current batch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_words(input_vec_list, vocab_size):\n",
    "    total_seq = len(input_vec_list)\n",
    "    max_len = max([len(x) for x in input_vec_list])\n",
    "    batched_input_list = []\n",
    "\n",
    "    for word in input_vec_list:\n",
    "        if max_len != len(word):\n",
    "            ## Add blanks to get max len\n",
    "            blank_vec = (vocab_size * np.ones((max_len - word.shape[0])))\n",
    "            word = np.concatenate((word, blank_vec), axis=0)\n",
    "        batched_input_list.append(word)\n",
    "\n",
    "    return np.array(batched_input_list)\n",
    "\n",
    "def get_cur_batch_data(\n",
    "    cur_epoch_data_list, \n",
    "    batch_id, \n",
    "    batch_size,\n",
    "    vocab_size\n",
    "):\n",
    "    if(((batch_id + 1) * batch_size) <= len(cur_epoch_data_list)):\n",
    "        start_index = (batch_id * batch_size)\n",
    "        end_index = ((batch_id + 1) * batch_size)\n",
    "        cur_batch_data_list = cur_epoch_data_list[start_index: end_index]\n",
    "    else:\n",
    "        start_index = (batch_id * batch_size)\n",
    "        end_index = len(cur_epoch_data_list)\n",
    "        cur_batch_data_list = cur_epoch_data_list[start_index: end_index]\n",
    "    \n",
    "    ## Convert to numpy arrays\n",
    "    word_length_array = np.array([len(x[0]) for x in cur_batch_data_list])\n",
    "    input_vec_list = [x[0] for x in cur_batch_data_list]\n",
    "    batched_input_array = batchify_words(input_vec_list, vocab_size)\n",
    "    batched_label_array = np.array([x[1] for x in cur_batch_data_list])\n",
    "    batched_missed_char_array = np.array([x[2] for x in cur_batch_data_list])\n",
    "\n",
    "    ## Return batch\n",
    "    return batched_input_array, batched_label_array, batched_missed_char_array, word_length_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Get current batch data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_array, batched_label_array, batched_missed_char_array, word_length_array = get_cur_batch_data(\n",
    "    cur_epoch_data_list = cur_epoch_train_data_list, \n",
    "    batch_id = 2, \n",
    "    batch_size = 4000,\n",
    "    vocab_size = 26\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000 4000 4000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(batched_input_array),\n",
    "    len(batched_label_array),\n",
    "    len(batched_missed_char_array),\n",
    "    len(word_length_array)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26., 26., 26., 26., 26., 11., 11.,  4., 26., 19., 26., 21.,  4.,\n",
       "        26., 26., 26., 26., 26., 26., 26., 26.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_input_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_label_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_missed_char_array[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 10, 11,  5, 12])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length_array[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "\tepoch,\n",
    "\tmodel,\n",
    "\ttotal_epochs,\n",
    "\tencoded_test_word_list,\n",
    "\tbatch_size,\n",
    "\tvocab_size,\n",
    "\tcuda\n",
    "):\n",
    "\tmodel.eval()\n",
    "\n",
    "\t## Initialize epoch loss\n",
    "\ttest_loss = 0.0\n",
    "\ttest_miss_penalty = 0.0\n",
    "\n",
    "\t## Without gradient update\n",
    "\twith torch.no_grad():\n",
    "\t\t## Get cur_epoch_train_data_list\n",
    "\t\tcur_epoch_test_data_list = get_current_epoch_data(\n",
    "\t\t\tencoded_word_list = encoded_test_word_list, \n",
    "\t\t\tepoch_number = epoch,\n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tvocab_size = vocab_size\n",
    "\t\t)\n",
    "\n",
    "\t\t## Loop over batches\n",
    "\t\tno_batches = int(math.ceil(len(cur_epoch_test_data_list) / batch_size))\n",
    "\t\tfor batch_id in range(no_batches):\n",
    "\t\t\t## Get batch\n",
    "\t\t\tinputs, labels, miss_chars, input_lengths = get_cur_batch_data(\n",
    "\t\t\t\tcur_epoch_data_list = cur_epoch_test_data_list, \n",
    "\t\t\t\tbatch_id = batch_id, \n",
    "\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\tvocab_size = vocab_size\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t## Embeddings should be of dtype long\n",
    "\t\t\tinputs = torch.from_numpy(inputs).long()\n",
    "\t\t\t\n",
    "\t\t\t## Convert to torch tensors\n",
    "\t\t\tlabels = torch.from_numpy(labels).float()\n",
    "\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
    "\t\t\tinput_lengths = torch.from_numpy(input_lengths).long()\n",
    "\n",
    "\t\t\tif(cuda==True):\n",
    "\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\tlabels = labels.cuda()\n",
    "\t\t\t\tmiss_chars = miss_chars.cuda()\n",
    "\t\t\t\tinput_lengths = input_lengths.cuda()\n",
    "\n",
    "\t\t\t# zero the parameter gradients\n",
    "\t\t\tmodel.optimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\t# Forward Pass\n",
    "\t\t\toutputs = model(inputs, input_lengths, miss_chars)\n",
    "\t\t\tloss, miss_penalty = model.calculate_loss(outputs, labels, input_lengths, miss_chars, cuda)\n",
    "\t\t\ttest_loss += loss.item()\n",
    "\t\t\ttest_miss_penalty += miss_penalty.item()\n",
    "\n",
    "\t# Average out the losses\n",
    "\ttest_loss = (test_loss / no_batches)\n",
    "\ttest_miss_penalty = (test_miss_penalty / no_batches)\n",
    "\treturn test_loss, test_miss_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bi Direnctional GRU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "\ttotal_epochs,\n",
    "\tencoded_train_words_location,\n",
    "\tencoded_test_words_location,\n",
    "\tbatch_size,\n",
    "\tvocab_size,\n",
    "\tcuda,\n",
    "\tsave_every,\n",
    "\tmodel_output_location,\n",
    "\tgru_hidden_dim = 512,\n",
    "\tgru_num_layers = 2,\n",
    "\tchar_embedding_dim = 128,\n",
    "\tmissed_char_linear_dim = 256,\n",
    "\tnn_hidden_dim = 256,\n",
    "\tgru_dropout = 0.3,\n",
    "\tlearning_rate = 0.0005\n",
    "):\n",
    "\t## Load model and set it to train mode\n",
    "\tmodel = HangmanGRU(\n",
    "\t\tvocab_size = vocab_size,\n",
    "        gru_hidden_dim = gru_hidden_dim,\n",
    "        gru_num_layers = gru_num_layers,\n",
    "        char_embedding_dim = char_embedding_dim,\n",
    "        missed_char_linear_dim = missed_char_linear_dim,\n",
    "        nn_hidden_dim = nn_hidden_dim,\n",
    "        gru_dropout = gru_dropout,\n",
    "        learning_rate = learning_rate\n",
    "\t)\n",
    "\tmodel.train()\n",
    "\n",
    "\t## Get encoded_train_word_list\n",
    "\tencoded_train_word_list = pickle.load(open(encoded_train_words_location, \"rb\"))\n",
    "\t\n",
    "\t## Get encoded_test_word_list\n",
    "\tencoded_test_word_list = pickle.load(open(encoded_test_words_location, \"rb\"))\n",
    "\n",
    "\t## Lists to store losses\n",
    "\ttrain_loss_list = []\n",
    "\ttrain_miss_penalty_list = []\n",
    "\ttest_loss_list = []\n",
    "\ttest_miss_penalty_list = []\n",
    "\n",
    "\t## Loop over Train Data\n",
    "\tfor epoch in range(1, total_epochs+1):\n",
    "\t\t## Initialize epoch loss\n",
    "\t\ttrain_loss = 0.0\n",
    "\t\ttrain_miss_penalty = 0.0\n",
    "\n",
    "\t\t## Get cur_epoch_train_data_list\n",
    "\t\tcur_epoch_train_data_list = get_current_epoch_data(\n",
    "\t\t\tencoded_word_list = encoded_train_word_list, \n",
    "\t\t\tepoch_number = epoch, \n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tvocab_size = vocab_size\n",
    "\t\t)\n",
    "\n",
    "\t\t## Loop over batches\n",
    "\t\tno_batches = int(math.ceil(len(cur_epoch_train_data_list) / batch_size))\n",
    "\t\tfor batch_id in range(no_batches):\n",
    "\t\t\t## Get batch\n",
    "\t\t\tinputs, labels, miss_chars, input_lengths = get_cur_batch_data(\n",
    "\t\t\t\tcur_epoch_data_list = cur_epoch_train_data_list, \n",
    "\t\t\t\tbatch_id = batch_id, \n",
    "\t\t\t\tbatch_size = batch_size,\n",
    "\t\t\t\tvocab_size = vocab_size\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\t## Embeddings should be of dtype long\n",
    "\t\t\tinputs = torch.from_numpy(inputs).long()\n",
    "\t\t\t\n",
    "\t\t\t## Convert to torch tensors\n",
    "\t\t\tlabels = torch.from_numpy(labels).float()\n",
    "\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
    "\t\t\tinput_lengths = torch.from_numpy(input_lengths).long()\n",
    "\n",
    "\t\t\tif(cuda==True):\n",
    "\t\t\t\tinputs = inputs.cuda()\n",
    "\t\t\t\tlabels = labels.cuda()\n",
    "\t\t\t\tmiss_chars = miss_chars.cuda()\n",
    "\t\t\t\tinput_lengths = input_lengths.cuda()\n",
    "\n",
    "\t\t\t## Zero the parameter gradients\n",
    "\t\t\tmodel.optimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\t## Forward Pass, Loss calculation, Backward Pass, Optimize\n",
    "\t\t\toutputs = model(inputs, input_lengths, miss_chars)\n",
    "\t\t\tloss, miss_penalty = model.calculate_loss(outputs, labels, input_lengths, miss_chars, cuda)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tmodel.optimizer.step()\n",
    "\n",
    "\t\t\t## store loss\n",
    "\t\t\ttrain_loss += loss.item()\n",
    "\t\t\ttrain_miss_penalty += miss_penalty.item()\n",
    "\n",
    "\t\t# Test model after epoch\n",
    "\t\ttest_loss, test_miss_penalty = test(\n",
    "\t\t\tepoch = epoch,\n",
    "\t\t\tmodel = model,\n",
    "\t\t\ttotal_epochs = total_epochs,\n",
    "\t\t\tencoded_test_word_list = encoded_test_word_list,\n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tvocab_size = vocab_size,\n",
    "\t\t\tcuda = cuda\n",
    "\t\t)\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# Store losses\n",
    "\t\ttrain_loss = (train_loss / no_batches)\n",
    "\t\ttrain_loss_list.append(train_loss)\n",
    "\t\ttrain_miss_penalty = (train_miss_penalty/ no_batches)\n",
    "\t\ttrain_miss_penalty_list.append(train_miss_penalty)\n",
    "\t\ttest_loss_list.append(test_loss)\n",
    "\t\ttest_miss_penalty_list.append(test_miss_penalty)\n",
    "\n",
    "\t\t# Save Losses\n",
    "\t\tdf_losses = pd.DataFrame(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"train_loss\": train_loss_list,\n",
    "\t\t\t\t\"train_miss_penalty\": train_miss_penalty_list,\n",
    "\t\t\t\t\"test_loss\": test_loss_list,\n",
    "\t\t\t\t\"test_miss_penalty\": test_miss_penalty_list\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\t\tdf_losses_location = f\"{model_output_location}/df_losses.csv\"\n",
    "\t\tdf_losses.to_csv(df_losses_location, index=False)\n",
    "\n",
    "\t\t# Save model\n",
    "\t\tif(epoch % save_every == 0):\n",
    "\t\t\tmodel_path = f\"{model_output_location}/models\"\n",
    "\t\t\tmodel_file_name = f\"{model_path}/model_epoch_{str(epoch).zfill(4)}.pth\"\n",
    "\t\t\ttorch.save({\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'model_state_dict': model.state_dict(),\n",
    "\t\t\t\t'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "\t\t\t\t'train_loss': train_loss,\n",
    "\t\t\t\t'test_loss': test_loss,\n",
    "\t\t\t}, model_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Bi Directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train(\n",
    "\ttotal_epochs = 10,\n",
    "\tencoded_train_words_location = \"data/encoded_train_words.pickle\",\n",
    "\tencoded_test_words_location = \"data/encoded_test_words.pickle\",\n",
    "\tbatch_size = 250000,\n",
    "\tvocab_size = 26,\n",
    "\tcuda = False,\n",
    "\tsave_every = 1,\n",
    "\tmodel_output_location = \"model_output\",\n",
    "\tgru_hidden_dim = 512,\n",
    "\tgru_num_layers = 2,\n",
    "\tchar_embedding_dim = 128,\n",
    "\tmissed_char_linear_dim = 256,\n",
    "\tnn_hidden_dim = 256,\n",
    "\tgru_dropout = 0.3,\n",
    "\tlearning_rate = 0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "270af51b3687a98de993e3398f2a00195eb6c674711263c21cd6f4db7277b321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
