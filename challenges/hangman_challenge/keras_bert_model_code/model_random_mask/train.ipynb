{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 45\n",
    "    BATCH_SIZE = 10000\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 29\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['aaa', 'aaaaaa'], 227300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Data\n",
    "data_location = \"../../data/words_250000_train.txt\"\n",
    "with open(data_location,'r') as f:\n",
    "    word_list = f.read().splitlines()\n",
    "word_list[:2], len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word_list(word_list, min_word_len = 2):\n",
    "    ## Strip Lowercase Remove < min_word_len  length\n",
    "    word_list = list(set(word_list))\n",
    "    lower_word_list = []\n",
    "    for word in word_list:\n",
    "        if(len(word) >= min_word_len):\n",
    "            lower_word_list.append(word.strip().lower())\n",
    "    lower_word_list[:2], len(lower_word_list)\n",
    "\n",
    "    ## Get max_train_word_length\n",
    "    # max_train_word_length = max([len(word) for word in lower_word_list])\n",
    "\n",
    "    # ## Prepare subword list\n",
    "    # subword_set = set()\n",
    "    # for cur_word_length in tqdm(range(min_word_len,  max_train_word_length+1)):\n",
    "    #     for word in lower_word_list:\n",
    "    #         if(len(word) >= cur_word_length):\n",
    "    #             for i in range(len(word) - cur_word_length + 1):\n",
    "    #                 subword_set.add(word[i: (i+cur_word_length)])\n",
    "    # subword_list = list(set(list(subword_set)))\n",
    "\n",
    "    ## Prepare sentence list\n",
    "    sentence_list = []\n",
    "    for word in lower_word_list:\n",
    "        sentence = \" \".join(list(word))\n",
    "        sentence_list.append(sentence)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['f o g l o g g e d', 's m o o d g i n g'], 227283)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = preprocess_word_list(word_list.copy())\n",
    "sentence_list[:2], len(sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vectorize layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 18:34:00.806328: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=config.VOCAB_SIZE,\n",
    "    standardize=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=config.MAX_LEN\n",
    ")\n",
    "vectorize_layer.adapt(sentence_list)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab = vocab[2 : config.VOCAB_SIZE - 1] + [\"_\"]\n",
    "vectorize_layer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'l',\n",
       " 'c',\n",
       " 'u',\n",
       " 'd',\n",
       " 'p',\n",
       " 'm',\n",
       " 'h',\n",
       " 'g',\n",
       " 'y',\n",
       " 'b',\n",
       " 'f',\n",
       " 'v',\n",
       " 'k',\n",
       " 'w',\n",
       " 'z',\n",
       " 'x',\n",
       " 'q',\n",
       " 'j',\n",
       " '_']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"_\"]).numpy()[0][0]\n",
    "mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encod Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded_sentence_array = encode(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227283, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20,  6, 17, ...,  0,  0,  0],\n",
       "       [ 8, 15,  6, ...,  0,  0,  0],\n",
       "       [13,  7,  4, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 7,  2,  8, ...,  0,  0,  0],\n",
       "       [ 8,  2,  7, ...,  0,  0,  0],\n",
       "       [17, 12, 10, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_sentence_array.shape)\n",
    "encoded_sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f - 20 | o - 6 | g - 17 | l - 10 | o - 6 | g - 17 | g - 17 | e - 2 | d - 13 | "
     ]
    }
   ],
   "source": [
    "for c in sentence_list[0].split(\" \"):\n",
    "    c_id = vectorize_layer([c]).numpy()[0][0]\n",
    "    print(f\"{c} - {c_id}\", end=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([20,  6, 17, 10,  6, 17, 17,  2, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoded_sentence_array[0]))\n",
    "encoded_sentence_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, ..., False,  True, False],\n",
       "       [ True, False, False, ...,  True,  True,  True],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False,  True,  True],\n",
       "       [ True, False, False, ..., False, False, False],\n",
       "       [False,  True, False, ...,  True,  True, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = np.random.rand(*encoded_sentence_array.shape) < 0.50\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get masked inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5593131015077699"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    mask_probability = random.uniform(0, 1)\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < mask_probability\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [mask] for inp_mask\n",
    "    encoded_texts_masked[inp_mask] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    if(i == 0):\n",
    "        x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(encoded_sentence_array)\n",
    "    else:\n",
    "        x_masked_train_, y_masked_labels_, sample_weights_ = get_masked_input_and_labels(encoded_sentence_array)\n",
    "        x_masked_train = np.concatenate((x_masked_train, x_masked_train_), axis=0)\n",
    "        y_masked_labels = np.concatenate((y_masked_labels, y_masked_labels_), axis=0)\n",
    "        sample_weights = np.concatenate((sample_weights, sample_weights_), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1136415, 45) (1136415, 45) (1136415, 45)\n"
     ]
    }
   ],
   "source": [
    "print(x_masked_train.shape, y_masked_labels.shape, sample_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(1136415, reshuffle_each_iteration=True).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlm_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BERT model (Pretraining Model) for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=2):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"c _ t\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MaskedTextGenerator at 0x7fa9ff427a50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 45, 128)      3712        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 45, 128)      0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 45, 128)      66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 45, 128)      0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 45, 128)      0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 45, 128)      256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 45, 128)      33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 45, 128)      0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 45, 128)      0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 45, 128)      256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mlm_cls (Dense)                 (None, 45, 29)       3741        encoder_0/ffn_layernormalization[\n",
      "==================================================================================================\n",
      "Total params: 107,037\n",
      "Trainable params: 107,037\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 19:01:08.859937: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1013s 9s/step - loss: 2.9047\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.23519625}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.18642396}\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 1068s 9s/step - loss: 2.7516\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.37396398}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.21297932}\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 1068s 9s/step - loss: 2.6032\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.32207373}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.3042035}\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 996s 9s/step - loss: 2.5294\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.37888426}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.28567722}\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 1042s 9s/step - loss: 2.4884\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.34287637}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.3270082}\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 1105s 10s/step - loss: 2.4613\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.38679233}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.27640194}\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 1016s 9s/step - loss: 2.4400\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.40750256}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.27163404}\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 965s 8s/step - loss: 2.4237\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.38898522}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.24800716}\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 976s 9s/step - loss: 2.4111\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.4086606}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.20147112}\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 989s 9s/step - loss: 2.4000\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.41456965}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.18625395}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa956e28a10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_masked_model.fit(\n",
    "    mlm_ds, \n",
    "    epochs=10, \n",
    "    callbacks=[generator_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_letters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_letter(model, id2token, token2id, word = \"b _ y\", special_tokens = [\"\", \"[UNK]\", \"_\"]):\n",
    "    clean_word = word.strip().lower()\n",
    "    \n",
    "    encoded_word = [token2id[c] for c in clean_word.split(\" \")]\n",
    "    len_word = len(encoded_word)\n",
    "    encoded_word = np.array(encoded_word)\n",
    "    encoded_word = np.pad(encoded_word, (0, 45-len_word))\n",
    "    encoded_word = encoded_word.reshape(1, 45)\n",
    "    \n",
    "    model_output = model.predict(encoded_word)[0]\n",
    "    \n",
    "    blank_index_list = [c_index for c_index, c in enumerate(clean_word.split(\" \")) if(c == \"_\")]\n",
    "    model_output = model_output[blank_index_list]\n",
    "    \n",
    "    model_output = np.max(model_output, axis = 0)\n",
    "    model_output = np.argsort(model_output)[::-1]\n",
    "    for id in model_output:\n",
    "        token = id2token[id]\n",
    "        if(token not in (guessed_letters + special_tokens)):\n",
    "            guessed_letters.append(token)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"b _ y\"\n",
    "predict_letter(bert_masked_model, id2token, token2id, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r', 'l', 'a', 'o']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guessed_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save required elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_masked_model.save(\"bert_mlm.h5\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2token, open(\"id2token.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token2id, open(\"token2id.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "270af51b3687a98de993e3398f2a00195eb6c674711263c21cd6f4db7277b321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
