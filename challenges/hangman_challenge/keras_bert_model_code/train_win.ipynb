{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 45\n",
    "    BATCH_SIZE = 10000\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 29\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'aaaaaa']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_location = \"../data/words_250000_train.txt\"\n",
    "with open(data_location,'r') as f:\n",
    "    word_list = f.read().splitlines()\n",
    "word_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word_list(word_list):\n",
    "    sentence_list = []\n",
    "    new_word_list = list(set(word_list))\n",
    "    for word in new_word_list:\n",
    "        if(len(word) < 3):\n",
    "            continue\n",
    "        sentence = word.strip().lower()\n",
    "        sentence = \" \".join(list(sentence))\n",
    "        sentence_list.append(sentence)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = preprocess_word_list(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a m b i t e n d e n c y', 'u n s h a r p e n i n g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vectorize layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    }
   ],
   "source": [
    "vocab_file_location = \"vocab.txt\"\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=config.VOCAB_SIZE,\n",
    "    standardize=None,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=config.MAX_LEN\n",
    ")\n",
    "vectorize_layer.adapt(sentence_list)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab = vocab[2 : config.VOCAB_SIZE - 1] + [\"_\"]\n",
    "vectorize_layer.set_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'l',\n",
       " 'c',\n",
       " 'u',\n",
       " 'd',\n",
       " 'p',\n",
       " 'm',\n",
       " 'h',\n",
       " 'g',\n",
       " 'y',\n",
       " 'b',\n",
       " 'f',\n",
       " 'v',\n",
       " 'k',\n",
       " 'w',\n",
       " 'z',\n",
       " 'x',\n",
       " 'q',\n",
       " 'j',\n",
       " '_']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"_\"]).numpy()[0][0]\n",
    "mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encod Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for masked language model\n",
    "encoded_sentence_array = encode(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227019, 45)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14,  2, 10, ...,  0,  0,  0],\n",
       "       [16,  3, 17, ...,  0,  0,  0],\n",
       "       [14,  4,  7, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 7,  2,  9, ...,  0,  0,  0],\n",
       "       [14,  7,  2, ...,  0,  0,  0],\n",
       "       [15, 12, 10, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_sentence_array.shape)\n",
    "encoded_sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p - 14 | e - 2 | l - 10 | a - 4 | r - 7 | g - 17 | i - 3 | "
     ]
    }
   ],
   "source": [
    "for c in sentence_list[0].split(\" \"):\n",
    "    c_id = vectorize_layer([c]).numpy()[0][0]\n",
    "    print(f\"{c} - {c_id}\", end=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14,  2, 10,  4,  7, 17,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoded_sentence_array[0]))\n",
    "encoded_sentence_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False,  True, False],\n",
       "       [ True,  True, False, ...,  True,  True, False],\n",
       "       [False,  True, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False,  True,  True, ..., False, False,  True],\n",
       "       [ True, False,  True, ..., False, False,  True],\n",
       "       [False, False,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_mask = np.random.rand(*encoded_sentence_array.shape) < 0.50\n",
    "inp_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get masked inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.50\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [mask] for inp_mask\n",
    "    encoded_texts_masked[inp_mask] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(encoded_sentence_array)\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices((x_masked_train, y_masked_labels, sample_weights))\n",
    "mlm_ds = mlm_ds.shuffle(250000, reshuffle_each_iteration=True).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BERT model (Pretraining Model) for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "    reduction=tf.keras.losses.Reduction.NONE\n",
    ")\n",
    "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "# id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "# token2id = {y: x for x, y in id2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "sample_tokens = vectorize_layer([\"c _ t\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MaskedTextGenerator at 0x7ff54b881f10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 45, 128)      3712        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 45, 128)      0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 45, 128)      66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 45, 128)      0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 45, 128)      0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 45, 128)      256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 45, 128)      33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 45, 128)      0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 45, 128)      0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 45, 128)      256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mlm_cls (Dense)                 (None, 45, 29)       3741        encoder_0/ffn_layernormalization[\n",
      "==================================================================================================\n",
      "Total params: 107,037\n",
      "Trainable params: 107,037\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model = create_masked_language_bert_model()\n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 22:31:53.735885: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 223s 9s/step - loss: 3.0912\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'n',\n",
      " 'prediction': 'c n t',\n",
      " 'probability': 0.124638356}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.1096268}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'r',\n",
      " 'prediction': 'c r t',\n",
      " 'probability': 0.095599815}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.08754144}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.08186258}\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 205s 9s/step - loss: 2.8898\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.16653284}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.12689862}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'n',\n",
      " 'prediction': 'c n t',\n",
      " 'probability': 0.104830734}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'r',\n",
      " 'prediction': 'c r t',\n",
      " 'probability': 0.10035587}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.08901903}\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 219s 9s/step - loss: 2.8669\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.17982197}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.16929579}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'n',\n",
      " 'prediction': 'c n t',\n",
      " 'probability': 0.103970274}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.10303046}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.09124402}\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 195s 8s/step - loss: 2.8519\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.22524881}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.20490663}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.10452116}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.09077328}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'n',\n",
      " 'prediction': 'c n t',\n",
      " 'probability': 0.08175957}\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 198s 9s/step - loss: 2.8354\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.30652413}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.25972074}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.11830932}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.0699442}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.051385224}\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 199s 9s/step - loss: 2.8074\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.29986373}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.23344687}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.15468785}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.067121506}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.064317435}\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 197s 9s/step - loss: 2.7568\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.28335476}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.2401145}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.14460559}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.08059665}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.07873929}\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 196s 9s/step - loss: 2.7043\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.27016294}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.21909447}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.111818925}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'r',\n",
      " 'prediction': 'c r t',\n",
      " 'probability': 0.102808475}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.10069828}\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 201s 9s/step - loss: 2.6648\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.27866697}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.2573399}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.11760491}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.10644452}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'r',\n",
      " 'prediction': 'c r t',\n",
      " 'probability': 0.08564775}\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 208s 9s/step - loss: 2.6342\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'o',\n",
      " 'prediction': 'c o t',\n",
      " 'probability': 0.31673384}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'a',\n",
      " 'prediction': 'c a t',\n",
      " 'probability': 0.2835397}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'i',\n",
      " 'prediction': 'c i t',\n",
      " 'probability': 0.09915927}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'h',\n",
      " 'prediction': 'c h t',\n",
      " 'probability': 0.09561878}\n",
      "{'input_text': 'c _ t',\n",
      " 'predicted mask token': 'u',\n",
      " 'prediction': 'c u t',\n",
      " 'probability': 0.07510036}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff52fc39850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_masked_model.fit(\n",
    "    mlm_ds, \n",
    "    epochs=10, \n",
    "    callbacks=[generator_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_letters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_letter(model, id2token, token2id, word = \"b _ y\", special_tokens = [\"\", \"[UNK]\", \"_\"]):\n",
    "    clean_word = word.strip().lower()\n",
    "    \n",
    "    encoded_word = [token2id[c] for c in clean_word.split(\" \")]\n",
    "    len_word = len(encoded_word)\n",
    "    encoded_word = np.array(encoded_word)\n",
    "    encoded_word = np.pad(encoded_word, (0, 45-len_word))\n",
    "    encoded_word = encoded_word.reshape(1, 45)\n",
    "    \n",
    "    model_output = model.predict(encoded_word)[0]\n",
    "    \n",
    "    blank_index_list = [c_index for c_index, c in enumerate(clean_word.split(\" \")) if(c == \"_\")]\n",
    "    model_output = model_output[blank_index_list]\n",
    "    \n",
    "    model_output = np.max(model_output, axis = 0)\n",
    "    model_output = np.argsort(model_output)[::-1]\n",
    "    for id in model_output:\n",
    "        token = id2token[id]\n",
    "        if(token not in (guessed_letters + special_tokens)):\n",
    "            guessed_letters.append(token)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"_ a\"\n",
    "predict_letter(bert_masked_model, id2token, token2id, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guessed_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save required elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-21 23:27:42.601431: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_mlm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bert_mlm/assets\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.save(\"bert_mlm\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2token, open(\"id2token.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(token2id, open(\"token2id.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"bert_mlm.h5\", custom_objects = {\"MaskedLanguageModel\": MaskedLanguageModel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = pickle.load(open(\"id2token.pickle\", \"rb\"))\n",
    "token2id = pickle.load(open(\"token2id.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_letter(model, id2token, token2id, word = \"b _ y\", special_tokens = [\"\", \"[UNK]\", \"_\"]):\n",
    "    clean_word = word.strip().lower()\n",
    "    \n",
    "    encoded_word = [token2id[c] for c in clean_word.split(\" \")]\n",
    "    len_word = len(encoded_word)\n",
    "    encoded_word = np.array(encoded_word)\n",
    "    encoded_word = np.pad(encoded_word, (0, 45-len_word))\n",
    "    encoded_word = encoded_word.reshape(1, 45)\n",
    "    \n",
    "    model_output = model.predict(encoded_word)[0]\n",
    "    \n",
    "    blank_index_list = [c_index for c_index, c in enumerate(clean_word.split(\" \")) if(c == \"_\")]\n",
    "    model_output = model_output[blank_index_list]\n",
    "    \n",
    "    model_output = np.max(model_output, axis = 0)\n",
    "    model_output = np.argsort(model_output)[::-1]\n",
    "    for id in model_output:\n",
    "        token = id2token[id]\n",
    "        if(token not in (guessed_letters + special_tokens)):\n",
    "            guessed_letters.append(token)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_letters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xd"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"a _\"\n",
    "predict_letter(model, id2token, token2id, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'r', 'l', 'v', 'm', 't']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guessed_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "270af51b3687a98de993e3398f2a00195eb6c674711263c21cd6f4db7277b321"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
